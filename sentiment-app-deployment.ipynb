{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Sentiment Analysis Web App\n",
    "### Pytorch and AWS SageMaker\n",
    "_SageMaker, Lambda, API, CloudWatch_\n",
    "\n",
    "---\n",
    "Put an overview of the notebook here\n",
    "\n",
    "## Outline\n",
    "1. [Download the data](#download)\n",
    "2. [Process and prepare the data](#process)\n",
    "3. [Upload data to S3](#upload)\n",
    "4. [Train a model](#train)\n",
    "5. [Test the trained model](#test)\n",
    "6. [Deploy the trained model](#deploy)\n",
    "7. [Use the deployed model for inference](#use)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='download'></a>\n",
    "## Download the Data\n",
    "\n",
    "The notebook and model use the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-03 17:35:31--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  11.6MB/s    in 6.5s    \n",
      "\n",
      "2020-08-03 17:35:38 (12.4 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='process'></a>\n",
    "## Process and Prepare the Data\n",
    "\n",
    "---\n",
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports \n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_imbd_data(data_dir='../data/aclImdb'):\n",
    "    \"\"\" Read in IMDb data from aclImdb folder. Creates data and label dictionaries.\n",
    "    \n",
    "        Arguments:\n",
    "        - data_dir: (str) Directory of the data\n",
    "        \n",
    "        Returns:\n",
    "        - data: (dict) Movie review\n",
    "        - labels: (dict) Movie review labels\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    # create paths to read in review data\n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            # join path names\n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            # open each review and label. Append to dictionaries and label with binary vars\n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                       \"{}: data size does not equal {}: label size.\".format(data_type, sentiment)\n",
    "                    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb Reviews: Train --> 12500 pos / 12500 neg ... Test --> 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "# read in data and display length of train and test data\n",
    "data, labels = read_imbd_data()\n",
    "print(\"IMDb Reviews: Train --> {} pos / {} neg ... Test --> {} pos / {} neg\".format(len(data['train']['pos']),\n",
    "                                                                                    len(data['train']['neg']),\n",
    "                                                                                    len(labels['test']['pos']),\n",
    "                                                                                    len(labels['test']['pos'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I didn't know what to make of this film. I guess that is what it was all about really. I have never seen a film like it and I doubt that I really ever will again. Glover puts together something that is unique to him. I think to appreciate it you have to read some of his poetry, maybe see one of his slide shows. I really like this guy, he is just so bizarre I can't help it. Note: I saw this film before it was through its final editing, so maybe what I have seen and what others have seen are different. I will know, I guess, if I choose to view the film again. I think I will have to be properly drug influenced...\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['pos'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['train']['pos'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Create Feature and Target Sets\n",
    "Combine the training and test data/labels and shuffle to creat feature and target sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_imdb_data(data, labels):\n",
    "    \"\"\" Combine pos and neg reviews from the training and test data \n",
    "        dictionaries.\n",
    "        \n",
    "        Arguments:\n",
    "        - data: (dict) Unprocessed reviews\n",
    "        - labels: (dict) Sentiment label, 1 pos --> 0 neg\n",
    "        \n",
    "        Returns:\n",
    "        - train_X, test_X: features\n",
    "        - train_y, test_y: targets\n",
    "    \"\"\"\n",
    "    # combine positive and negative reviews and labels\n",
    "    train_data = data['train']['pos'] + data['train']['neg']\n",
    "    test_data = data['test']['pos'] + data['test']['neg']\n",
    "    train_labels = labels['train']['pos'] + labels['train']['neg']\n",
    "    test_labels = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    # using sklearn shuffle data\n",
    "    train_data, train_labels = shuffle(train_data, train_labels)\n",
    "    test_data, test_labels = shuffle(test_data, test_labels)\n",
    "    \n",
    "    return train_data, test_data, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb Data Length: Train data = 25000, Test data = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = combine_imdb_data(data, labels)\n",
    "print(\"IMDb Data Length: Train data = {}, Test data = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cary Grant and Myrna Loy are perfectly cast as a middle class couple who want to build the house of their dreams. It all starts out with reasonable plans and expectations, both of which are blown to bits by countless complications and an explosion of the original budget.<br /><br />There are many great laughs (even if the story is somewhat thin) sure to entertain fans of the stars or the late 1940s Hollywood comedy style. A definite highlight comes when a contractor goes through a run down of all expenses, which must have sounded quite excessive to a 1948 audience. As he makes his exit, he assures the client (Grant) that perhaps he could achieve a reduction of $100.00 from the total...or at least $50.00...but certainly $25.00. Hilarious! \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# take a look at a review and it's corresponding label\n",
    "print(train_X[20], '\\n')\n",
    "print(train_y[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Process Review\n",
    "Remove the html formatting and convert the review into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(review):\n",
    "    \"\"\" Converts a review string to a list of words. Removes html\n",
    "        formatting, stopwords and morphological endings of common\n",
    "        words.\n",
    "        \n",
    "        Arguments:\n",
    "        - review: (str) String of words that make up review\n",
    "        \n",
    "        Returns:\n",
    "        - words: (list) List of processed words in a review\n",
    "    \n",
    "    \"\"\" \n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(review, 'html.parser').get_text() # remove html tags\n",
    "    text = re.sub(r\"[^a-zA-z0-9]\", \" \", text.lower()) \n",
    "    words = text.split() # split the string into a list of words\n",
    "    words = [word for word in words if word not in stopwords.words('english')] # remove stopwords\n",
    "    words = [stemmer.stem(word) for word in words] # stem words\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cari', 'grant', 'myrna', 'loy', 'perfectli', 'cast', 'middl', 'class', 'coupl', 'want', 'build', 'hous', 'dream', 'start', 'reason', 'plan', 'expect', 'blown', 'bit', 'countless', 'complic', 'explos', 'origin', 'budget', 'mani', 'great', 'laugh', 'even', 'stori', 'somewhat', 'thin', 'sure', 'entertain', 'fan', 'star', 'late', '1940', 'hollywood', 'comedi', 'style', 'definit', 'highlight', 'come', 'contractor', 'goe', 'run', 'expens', 'must', 'sound', 'quit', 'excess', '1948', 'audienc', 'make', 'exit', 'assur', 'client', 'grant', 'perhap', 'could', 'achiev', 'reduct', '100', '00', 'total', 'least', '50', '00', 'certainli', '25', '00', 'hilari']\n"
     ]
    }
   ],
   "source": [
    "words = review_to_words(train_X[20])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")\n",
    "os.makedirs(cache_dir, exist_ok=True) \n",
    "\n",
    "def preprocess_data(train_data, test_data, train_labels, test_labels,\n",
    "                    cache_dir=cache_dir, cache_file='preprocesssed_data.pkl'):\n",
    "    \"\"\" Convert each review to words and read from the cache file if \n",
    "        available. \n",
    "    \n",
    "    \"\"\"\n",
    "    # if cache file exists try to read from it\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f: # open and read binary file\n",
    "                cache_data = pickle.load(f)\n",
    "                print(\"Reading preprocessed data from cache file: {}\".format(cache_file))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # if cache data does not exist create it\n",
    "    if cache_data is None:\n",
    "        # process data to create list of words for each review\n",
    "        train_words = [review_to_words(review) for review in train_data]\n",
    "        test_words = [review_to_words(review) for review in test_data]\n",
    "        \n",
    "        # write to cache file if it doesn't exist\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(train_words=train_words, test_words=test_words,\n",
    "                              train_labels=train_labels, test_labels=test_labels)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file: {}\".format(cache_file))\n",
    "            \n",
    "    else:\n",
    "        # unpack data from cache file\n",
    "        train_words = cache_data['train_words']\n",
    "        test_words = cache_data['test_words']\n",
    "        train_labels = cache_data['train_labels']\n",
    "        test_labels = cache_data['test_labels']\n",
    "        \n",
    "    return train_words, test_words, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading preprocessed data from cache file: preprocesssed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['could', 'anyon', 'pleas', 'stop', 'john', 'carpent', 'continu', 'deliber', 'ruin', 'reput', 'low', 'go', 'seem', 'man', 'lost', 'self', 'respect', 'episod', 'look', 'like', 'done', 'film', 'student', 'even', 'worth', 'begin', 'talk', 'bad', 'borefest', 'direct', 'somebodi', 'talent', 'filmmak', 'without', 'motiv', 'come', 'mr', 'carpent', 'pleas', 'retir', 'immedi', 'rest', 'self', 'esteem', 'stop', 'spill', 'trash', 'like', 'bad', 'tradit', 'escap', 'l', 'ghost', 'mar', 'get', 'drunk', 'instead']\n"
     ]
    }
   ],
   "source": [
    "len(train_X[20])\n",
    "print(train_X[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Transform the Data\n",
    "First we will create a working vocabulary of the most frequently occuring words in our dataset. We will remove the words that occur most infrequently. Each review will be fixed in size with shorter reviews padded with zeros. This will allow our RNN to train more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(data, vocab_size=5000):\n",
    "    \"\"\" Construct and return a dictionary mapping each of the most frequently \n",
    "        appearing words to a unique integer.\n",
    "        \n",
    "        Arguments:\n",
    "        - data: preprocessed reviews\n",
    "        - vacab_size: (int) size of vacabulary\n",
    "        \n",
    "        Returns:\n",
    "        - word_dict: dictionary of vocabulary mappings\n",
    "    \"\"\"\n",
    "    # count and sort the words \n",
    "    word_count = Counter(np.concatenate(train_X, axis = 0))\n",
    "    sorted_vocab = sorted(word_count, key=word_count.get, reverse=True)\n",
    "    \n",
    "    # create a word dictionary\n",
    "    word_dict = {word: idx+2 for idx, word in enumerate(sorted_vocab[:vocab_size-2])}\n",
    "    \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the dictionary to make sure everything looks good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movi', 'film', 'one', 'like', 'time']\n"
     ]
    }
   ],
   "source": [
    "most_freq = [key for idx, (key, val) in enumerate(word_dict.items()) if idx < 5]\n",
    "print(most_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='upload'></a>\n",
    "## Upload the Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train'></a>\n",
    "## Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test'></a>\n",
    "## Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deploy'></a>\n",
    "## Deploy the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='use'></a>\n",
    "## Use the Deployed Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
